```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
# pandas for dataframe manipulation
# numpy will be using for numerical analysis
# matplotlib and seaborn for data plotting and visualization
```


```python
car_df = pd.read_csv('Car_Purchasing_Data.csv',encoding='ISO-8859-1')
```


```python
car_df[0:5]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Customer Name</th>
      <th>Customer e-mail</th>
      <th>Country</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Salary</th>
      <th>Credit Card Debt</th>
      <th>Net Worth</th>
      <th>Car Purchase Amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Martina Avila</td>
      <td>cubilia.Curae.Phasellus@quisaccumsanconvallis.edu</td>
      <td>Bulgaria</td>
      <td>0</td>
      <td>41.851720</td>
      <td>62812.09301</td>
      <td>11609.380910</td>
      <td>238961.2505</td>
      <td>35321.45877</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Harlan Barnes</td>
      <td>eu.dolor@diam.co.uk</td>
      <td>Belize</td>
      <td>0</td>
      <td>40.870623</td>
      <td>66646.89292</td>
      <td>9572.957136</td>
      <td>530973.9078</td>
      <td>45115.52566</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Naomi Rodriquez</td>
      <td>vulputate.mauris.sagittis@ametconsectetueradip...</td>
      <td>Algeria</td>
      <td>1</td>
      <td>43.152897</td>
      <td>53798.55112</td>
      <td>11160.355060</td>
      <td>638467.1773</td>
      <td>42925.70921</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Jade Cunningham</td>
      <td>malesuada@dignissim.com</td>
      <td>Cook Islands</td>
      <td>1</td>
      <td>58.271369</td>
      <td>79370.03798</td>
      <td>14426.164850</td>
      <td>548599.0524</td>
      <td>67422.36313</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cedric Leach</td>
      <td>felis.ullamcorper.viverra@egetmollislectus.net</td>
      <td>Brazil</td>
      <td>1</td>
      <td>57.313749</td>
      <td>59729.15130</td>
      <td>5358.712177</td>
      <td>560304.0671</td>
      <td>55915.46248</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Step 2 : Basic Visualization of dataset
```


```python
sns.pairplot(car_df)
```




    <seaborn.axisgrid.PairGrid at 0x23ab7cda4c0>




    
![png](output_5_1.png)
    



```python
 X = car_df.drop(['Customer Name','Customer e-mail','Country','Car Purchase Amount'],axis=1)
```


```python
X
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Salary</th>
      <th>Credit Card Debt</th>
      <th>Net Worth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>41.851720</td>
      <td>62812.09301</td>
      <td>11609.380910</td>
      <td>238961.2505</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>40.870623</td>
      <td>66646.89292</td>
      <td>9572.957136</td>
      <td>530973.9078</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>43.152897</td>
      <td>53798.55112</td>
      <td>11160.355060</td>
      <td>638467.1773</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>58.271369</td>
      <td>79370.03798</td>
      <td>14426.164850</td>
      <td>548599.0524</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>57.313749</td>
      <td>59729.15130</td>
      <td>5358.712177</td>
      <td>560304.0671</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>495</th>
      <td>0</td>
      <td>41.462515</td>
      <td>71942.40291</td>
      <td>6995.902524</td>
      <td>541670.1016</td>
    </tr>
    <tr>
      <th>496</th>
      <td>1</td>
      <td>37.642000</td>
      <td>56039.49793</td>
      <td>12301.456790</td>
      <td>360419.0988</td>
    </tr>
    <tr>
      <th>497</th>
      <td>1</td>
      <td>53.943497</td>
      <td>68888.77805</td>
      <td>10611.606860</td>
      <td>764531.3203</td>
    </tr>
    <tr>
      <th>498</th>
      <td>1</td>
      <td>59.160509</td>
      <td>49811.99062</td>
      <td>14013.034510</td>
      <td>337826.6382</td>
    </tr>
    <tr>
      <th>499</th>
      <td>1</td>
      <td>46.731152</td>
      <td>61370.67766</td>
      <td>9391.341628</td>
      <td>462946.4924</td>
    </tr>
  </tbody>
</table>
<p>500 rows Ã— 5 columns</p>
</div>




```python
y=car_df['Car Purchase Amount']
```


```python
y
```




    0      35321.45877
    1      45115.52566
    2      42925.70921
    3      67422.36313
    4      55915.46248
              ...     
    495    48901.44342
    496    31491.41457
    497    64147.28888
    498    45442.15353
    499    45107.22566
    Name: Car Purchase Amount, Length: 500, dtype: float64




```python
X.shape
```




    (500, 5)




```python
y.shape
```




    (500,)




```python
# NORMALIZING THE DATA TO MAKE IT RANGE FROM O TO 1 , WHICH IMPORVES THE PERFORMANCE OF OUR NETWORK

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```


```python
X_scaled
```




    array([[0.        , 0.4370344 , 0.53515116, 0.57836085, 0.22342985],
           [0.        , 0.41741247, 0.58308616, 0.476028  , 0.52140195],
           [1.        , 0.46305795, 0.42248189, 0.55579674, 0.63108896],
           ...,
           [1.        , 0.67886994, 0.61110973, 0.52822145, 0.75972584],
           [1.        , 0.78321017, 0.37264988, 0.69914746, 0.3243129 ],
           [1.        , 0.53462305, 0.51713347, 0.46690159, 0.45198622]])




```python
scaler.data_max_
```




    array([1.e+00, 7.e+01, 1.e+05, 2.e+04, 1.e+06])




```python
scaler.data_min_
```




    array([    0.,    20., 20000.,   100., 20000.])




```python
y = y.values.reshape(-1,1)
```


```python
y.shape
```




    (500, 1)




```python
X.shape
y_scaled = scaler.fit_transform(y)
```

    D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\sklearn\base.py:441: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names
      warnings.warn(
    


```python
# TRAINING OF OUR MODEL STARTS FROM HERE
```


```python
from sklearn.model_selection import train_test_split
```


```python
X_train,X_test,y_train,y_test = train_test_split(X_scaled,y_scaled,test_size=0.25)
```


```python
X_train.shape
```




    (375, 5)




```python
X_test.shape
```




    (125, 5)




```python
import tensorflow.keras
from keras.models import Sequential 
from keras.layers import Dense


model = Sequential()

# my neurons (hidden layer) , inputs , activation
model.add(Dense(5 , input_dim = 5 , activation = 'relu'))
model.add(Dense(5 , activation ='relu'))
model.add(Dense(1 , activation = 'linear'))

```


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 5)                 30        
    _________________________________________________________________
    dense_1 (Dense)              (None, 5)                 30        
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 6         
    =================================================================
    Total params: 66
    Trainable params: 66
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model.compile(optimizer = 'adam' , loss = 'mean_squared_error')
```


```python
epochs_hist = model.fit(X_train , y_train , epochs = 100 , batch_size = 50 , verbose = 1 , validation_split = 0.2)
```

    Epoch 1/100
    6/6 [==============================] - 0s 37ms/step - loss: 0.8363 - val_loss: 0.7675
    Epoch 2/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.7538 - val_loss: 0.6885
    Epoch 3/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.6746 - val_loss: 0.6136
    Epoch 4/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.6005 - val_loss: 0.5432
    Epoch 5/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.5304 - val_loss: 0.4792
    Epoch 6/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.4655 - val_loss: 0.4194
    Epoch 7/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.4019 - val_loss: 0.3666
    Epoch 8/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.3437 - val_loss: 0.3175
    Epoch 9/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.2913 - val_loss: 0.2733
    Epoch 10/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.2399 - val_loss: 0.2338
    Epoch 11/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.1954 - val_loss: 0.1950
    Epoch 12/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.1557 - val_loss: 0.1599
    Epoch 13/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.1210 - val_loss: 0.1278
    Epoch 14/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0943 - val_loss: 0.1014
    Epoch 15/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0739 - val_loss: 0.0807
    Epoch 16/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0648
    Epoch 17/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0465 - val_loss: 0.0529
    Epoch 18/100
    6/6 [==============================] - 0s 7ms/step - loss: 0.0383 - val_loss: 0.0440
    Epoch 19/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0323 - val_loss: 0.0373
    Epoch 20/100
    6/6 [==============================] - 0s 8ms/step - loss: 0.0277 - val_loss: 0.0323
    Epoch 21/100
    6/6 [==============================] - 0s 7ms/step - loss: 0.0243 - val_loss: 0.0286
    Epoch 22/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0216 - val_loss: 0.0259
    Epoch 23/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0239
    Epoch 24/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.0223
    Epoch 25/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.0211
    Epoch 26/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0201
    Epoch 27/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0193
    Epoch 28/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.0187
    Epoch 29/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0182
    Epoch 30/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0178
    Epoch 31/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0174
    Epoch 32/100
    6/6 [==============================] - ETA: 0s - loss: 0.018 - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0170
    Epoch 33/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0167
    Epoch 34/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0164
    Epoch 35/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0161
    Epoch 36/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0159
    Epoch 37/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0157
    Epoch 38/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0155
    Epoch 39/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0152
    Epoch 40/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0150
    Epoch 41/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149
    Epoch 42/100
    6/6 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0147
    Epoch 43/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0145
    Epoch 44/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0144
    Epoch 45/100
    6/6 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0142
    Epoch 46/100
    6/6 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0140
    Epoch 47/100
    6/6 [==============================] - 0s 7ms/step - loss: 0.0119 - val_loss: 0.0139
    Epoch 48/100
    6/6 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0138
    Epoch 49/100
    6/6 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0137
    Epoch 50/100
    6/6 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 0.0135
    Epoch 51/100
    6/6 [==============================] - 0s 10ms/step - loss: 0.0115 - val_loss: 0.0133
    Epoch 52/100
    6/6 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0132
    Epoch 53/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0131
    Epoch 54/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0130
    Epoch 55/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0128
    Epoch 56/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0111 - val_loss: 0.0127
    Epoch 57/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0110 - val_loss: 0.0126
    Epoch 58/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0109 - val_loss: 0.0125
    Epoch 59/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0108 - val_loss: 0.0123
    Epoch 60/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0122
    Epoch 61/100
    6/6 [==============================] - 0s 16ms/step - loss: 0.0106 - val_loss: 0.0121
    Epoch 62/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 0.0120
    Epoch 63/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0119
    Epoch 64/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0118
    Epoch 65/100
    6/6 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0117
    Epoch 66/100
    6/6 [==============================] - 0s 4ms/step - loss: 0.0102 - val_loss: 0.0115
    Epoch 67/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0114
    Epoch 68/100
    6/6 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0113
    Epoch 69/100
    6/6 [==============================] - 0s 14ms/step - loss: 0.0099 - val_loss: 0.0112
    Epoch 70/100
    6/6 [==============================] - 0s 14ms/step - loss: 0.0099 - val_loss: 0.0111
    Epoch 71/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0098 - val_loss: 0.0110
    Epoch 72/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0097 - val_loss: 0.0108
    Epoch 73/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0096 - val_loss: 0.0107
    Epoch 74/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0095 - val_loss: 0.0106
    Epoch 75/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0094 - val_loss: 0.0106
    Epoch 76/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0094 - val_loss: 0.0105
    Epoch 77/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0093 - val_loss: 0.0104
    Epoch 78/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0092 - val_loss: 0.0103
    Epoch 79/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0092 - val_loss: 0.0102
    Epoch 80/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0091 - val_loss: 0.0101
    Epoch 81/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0090 - val_loss: 0.0100
    Epoch 82/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0089 - val_loss: 0.0099
    Epoch 83/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0098
    Epoch 84/100
    6/6 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0097
    Epoch 85/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0096
    Epoch 86/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0095
    Epoch 87/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0094
    Epoch 88/100
    6/6 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0093
    Epoch 89/100
    6/6 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0093
    Epoch 90/100
    6/6 [==============================] - 0s 11ms/step - loss: 0.0084 - val_loss: 0.0092
    Epoch 91/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0084 - val_loss: 0.0091
    Epoch 92/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0083 - val_loss: 0.0090
    Epoch 93/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0082 - val_loss: 0.0089
    Epoch 94/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0082 - val_loss: 0.0088
    Epoch 95/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0081 - val_loss: 0.0087
    Epoch 96/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0080 - val_loss: 0.0087
    Epoch 97/100
    6/6 [==============================] - 0s 14ms/step - loss: 0.0080 - val_loss: 0.0086
    Epoch 98/100
    6/6 [==============================] - 0s 13ms/step - loss: 0.0079 - val_loss: 0.0085
    Epoch 99/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.0084
    Epoch 100/100
    6/6 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.0083
    


```python
epochs_hist.history.keys()
```




    dict_keys(['loss', 'val_loss'])




```python
plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])
plt.title('Model Loss Progress During Training')
plt.ylabel('Training and Validation Loss')
plt.xlabel('Epoch number')
plt.legend(['Training Loss' , 'Validation Loss'])
```




    <matplotlib.legend.Legend at 0x23abf02c2e0>




    
![png](output_29_1.png)
    



```python
# Gender , Age , Annual Salary , Credit Card debt , Net worth

X_test = np.array([[2, 30 , 50000 , 100000 , 600000]])

y_predict = model.predict(X_test)


```


```python
print("Expected Purchase amount of a client after analyizing the data ", y_predict)
```

    Expected Purchase amount of a client after analyizing the data  [[412571.25]]
    


```python

```
